<!DOCTYPE html>
<html>
<head>
	  <meta charset="UTF-8" />
	  <meta name="viewport" content="width=device-width, initial-scale=1">
	  <title>SCEC 2017 - First Workshop on Software Challenges to Exascale Computing (SCEC)</title>
	  <meta name="keywords" content="HPC, Supercomputing, Exascale, Software, SCEC, SCEC17, Advanced Software Engineering">
	  <meta name="description" content="SCEC17 - First Workshop on Software Challenges to Exascale Computing">
	  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
	  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
	  <link rel="stylesheet" type="text/css" href="style.css">
	  <link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
	  <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
	  <link href="https://fonts.googleapis.com/css?family=Merriweather" rel="stylesheet">
</head>

<body>
		<!-- Background for top part -->
		<header class="topSectionBG">
	  </header>
		<!-- Wrapper for all texts on the top part -->
		<div class="topSectionWrapper">
				<!-- Navbar size fits the screen -->
	  		<nav class="navbar navbar-fixed-top longNavbar" id="navbarTop">
					<a class="navbar-brand generousSponsor" href="#">Genorously sponsored by</a>
					<a href="https://www.intel.com/" target="_blank"><img src="../img/intel_small_logo.jpg" class="intelLogo" id="theIntelLogo" alt="intel"></a>
						<ul class="nav navbar-nav navbar-right longNavbar">
								<li id="overwiewOption"><a href="#">Overview</a></li>
								<li id="topicsOption"><a href="#">Topics</a></li>
								<li id="agendaOption"><a href="#">Agenda</a></li>
								<li id="datesOption"><a href="#">Dates</a></li>
								<li id="committeeOption"><a href="#">Committee</a></li>
								<li id="sponsorsOption"><a href="#">Sponsors</a></li>
								<li id="CFAOption"><a href="#">CFA</a></li>
								<li id="registrationOption"><a href="#">Registration</a></li>
								<li id="contactOption"><a href="#">Contact</a></li>
						</ul>
				</nav>
				<!-- Navbar collapses since it does not fit the screen -->
				<nav class="navbar navbar-inverse navbar-fixed-top shortNavbar">
			 			<div class="container-fluid">
			 				<div class="navbar-header">
								<a class="navbar-brand hideBelow409" href="#">Generously sponsored by</a>
								<a href="https://www.intel.com/" target="_blank"><img src="../img/intel.jpg" class="intelLogo1" alt="intel"></a>
			 						<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
			 							<span class="icon-bar"></span>
			 							<span class="icon-bar"></span>
			 							<span class="icon-bar"></span>
			 						</button>
			 				</div>
			 					<div class="collapse navbar-collapse" id="myNavbar">
			 						<ul class="navbar-right">
										<li id="overwiewOption"><a href="#">Overview</a></li>
										<li id="topicsOption"><a href="#">Topics</a></li>
										<li id="agendaOption"><a href="#">Agenda</a></li>
										<li id="datesOption"><a href="#">Dates</a></li>
										<li id="committeeOption"><a href="#">Committee</a></li>
										<li id="sponsorsOption"><a href="#">Sponsors</a></li>
										<li id="CFAOption"><a href="#">CFA</a></li>
										<li id="registrationOption"><a href="#">Registration</a></li>
										<li id="contactOption"><a href="#">Contact</a></li>
			 						</ul>
			 					</div>
			 			</div>
			 		</nav>


 		<div class="container-fluid hidden-xs hidden-sm">
 			<div class="eventTitle">
 				<h1 class="text-center">First Workshop on Software Challenges to Exascale Computing</h1>
 				<div class="eventTitleDetails container">
 					<div class="col-md-2"></div>
 					<div class="col-md-8">
 						<div class="date container">
 								<div class="fa fa-calendar fa-3x calendar" aria-hidden="true"></div>
 								<div class="specificDate">
 									<b>December 17, 2017</b><br /><b>9:00 AM - 7:00 PM</b>
 								</div>
 						</div>
 						<div class="seperator"></div>
						<a href="https://www.google.com/maps/place/Hotel+Royal+Orchid/@26.8449021,75.793041,15z/data=!4m5!3m4!1s0x0:0x6bfe7d1ce5d6c4e!8m2!3d26.8449021!4d75.793041" target="_blank">
 						<div class="location container">
 								<div class="fa fa-map-marker fa-3x locationLogo" aria-hidden="true"></div>
 								<div class="specificDate left-aligned">
									Hotel Royal Orchid <br />
 									Opposite to BSNL Office, Tonk Road, Durgapura, Jaipur, Rajasthan, India
 								</div>
 						</div>
						</a>
 					</div>
 					<div class="col-md-2"></div>
 				</div>
 			</div>
 		</div>

 		<div class="container-fluid hidden-md hidden-lg hidden-xl">
 			<div class="eventTitle1">
 				<h1 class="text-center">First Workshop on Software Challenges to Exascale Computing</h1>
 				<h2 class="text-center">December 17, 2017<br /><b>9:30 AM - 5:30 PM</b><br/>
 				Jaipur, Rajasthan, India</a></h2>
 			</div>
 		</div>
 		</div>

		<a href="#" name="overwiewOption"></a>
			<div class="agenda"><br /></div>
		     <div class="container-fluid overviewSec">
		            <div class="titleScec">
		            	<div class="row">
		            		<h1 class="bold">SCEC 2017</h1>
		            	</div>
		            </div>
		          	<div class="col-md-6">
		          	<h3 class="bold"> Overview</h3>
		          	<p class="fontType justified">Supercomputing or High Performance Computing (HPC) platforms are used to power discoveries and to reduce the time-to-results in a wide variety of disciplines (such as, astrophysics, archaeology, and financial trading). For optimally utilizing these high-end platforms, it is critical to have scalable and efficient software (applications, middleware, libraries, and tools) that can take advantage of the innovative hardware features in these platforms. However, developing and maintaining HPC software remains a challenging task because the HPC platforms for which they are developed typically have a short life-span, and are replaced with next generation platforms within a few years. As we progress towards the exascale computing era, the task of developing and maintaining HPC software is likely to become even more challenging than now due to the increasing complexity of the HPC platforms and the pressing need for power-efficiency and memory usage optimization. There is a potential of mitigating some of the challenges related to developing HPC software for the current and future generation systems by adopting the innovations in the advanced software engineering sub-disciplines, such as model-driven engineering, generative programming, and adaptive and reflective software systems.</p>
								<br/>
		          	<p class="fontType justified">The goals of the first workshop on “Software Challenges to Exascale Computing” are to foster international collaborations across the HPC and the advanced software engineering disciplines, and to exchange knowledge on the challenges and solution strategies for developing scalable and efficient HPC software. The workshop attendees will learn about the state-of-the-art and the state-of-the-practice in the areas of HPC software development and advanced software engineering through presentations, hands-on sessions, and open-discussion sessions. Those already skilled in the advanced software engineering discipline will learn about the challenges and opportunities in the HPC domain, and can find interesting test cases for generalizing their innovative approaches.</p>
		            </div>
		            <div class="col-md-6">
		            <h3 class="bold remove"> Overview</h3>
		            <p class="fontType justified">The workshop will provide a forum through which hardware vendors and software developers can communicate with each other and influence the architecture of the next generation supercomputing systems and the supporting software stack. By fostering cross-disciplinary associations, the workshop will serve as a stepping stone towards innovations in the future. <br/><br />
		          	Benefits to the researchers and users in the academia: disseminate your results to the public, and find potential collaborators. <br /> <br />
								Benefits to software developers: understand the future trends in the HPC hardware and develop collaborations in the code modernization and optimization disciplines.<br /> <br />
								Benefits to HPC service providers: understand the challenges that the community faces in using the HPC platforms efficiently, and connect with the user-community. <br /><br />
								Benefits to HPC hardware vendors: understand the evolving needs of the HPC community, and network with potential customers.<br /><br />
								Benefits to students: network with HPC and advanced software engineering professionals and researchers, learn about internship and career opportunities, discuss the opportunities for higher education.
								</p>
							</div>
		         </div>


		      <a name="topicsOption"></a>
					<div class="onlyBorder">
		        <div class="container-fluid topicsSec">
		        	<div class="theBorder container">
		            <h3 class="bold"><u><br>Topics of interest include, but are not limited to:</u></h3>
		            <ul class="noList fontType">
		                <li>Best practices for HPC software development</li>
		                <li>Software tools for HPC code modernization and optimization</li>
		                <li>Supporting software and middleware for HPC environments</li>
		                <li>High-level interfaces, libraries, compilers, and runtime systems for parallel programming </li>
		                <li>Innovations in efficient utilization of memory hierarchies on HPC platforms</li>
		                <li>Techniques for developing power-efficient applications </li>
		                <li>Software support for HPC in the cloud</li>
				<li>Application of generative programming and model-driven engineering techniques for solving large-scale problems</li>
			        <li>Design and development of adaptive and reflective systems</li>
		            </ul>
		        </div >
						<br />
		      </div>
					</div>

		<a name="agendaOption"></a>
				<div class="onlyBorder2">
	        <div class="container-fluid topicsSec">
		        	<div class="theBorder container">
									  <div class="col-xs-12 hideAgendaTitle-md">
												<div class="col-xs-1"></div>
												<div class="col-xs-10">
														<br />
														<h1 class="bold">Tentative Agenda (Subject to Change)</h1>
												</div>
												<div class="col-xs-1"></div>
										</div>
										<div class="hideAgendaTitle-lg-sm1 hideAgendaTitle-lg-sm">
												<br />
												<h1 class="bold">Tentative Agenda (Subject to Change)</h1>
										</div>
										<div class="hideAgendaTitle-lg1 hideAgendaTitle-max477">
												<br />
												<h1 class="bold">Tentative Agenda <br />(Subject to Change)</h1>
										</div>
										<div class="hideAgendaTitle-min477 hideAgendaTitle-max399">
												<br />
												<h2 class="bold">Tentative Agenda <br />(Subject to Change)</h1>
										</div>
										<div class="hideAgendaTitle-min399">
												<br />
												<h4 class="bold">Tentative Agenda <br />(Subject to Change)</h1>
										</div>
										<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2 bold">Time
											</div>
											<div class="col-xs-8 bold left-aligned">Topic
											</div>
											<div class="col-xs-2 bold left-aligned">Speaker
											</div>
										</div>
										<div class="col-xs-12 hideAbove500 styleFont1">
												<div class="col-xs-2 bold paddingLeftEventTime">Time
												</div>
												<div class="col-xs-8 bold left-aligned paddingLeftEventTitle">Topic
												</div>
												<div class="col-xs-2 bold left-aligned paddingLeftSpeaker">Speaker
												</div>
										</div>
									<div class="col-xs-12"><hr /></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">09:00-09:30
											</div>
											<div class="col-xs-8 left-aligned">Registration, badges, kit-bag pick-up
											</div>
											<div class="col-xs-2 left-aligned">Location: TBD
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">09:00-09:30
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">Registration, badges, kit-bag pick-up
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">Location: TBD
											</div>
									</div>
								<div class="col-xs-12"><hr /></div>
								<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">09:30-10:00
											</div>
											<div class="col-xs-8 left-aligned">Welcome to the workshop - overview & goals
											</div>
											<div class="col-xs-2 left-aligned">Ritu Arora, TACC/ Sharda Dixit, C-DAC
											</div>
								</div>
								<div class="col-xs-12 hideAbove500 styleFont">
										<div class="col-xs-2 paddingLeftEventTime">09:30-10:00
										</div>
										<div class="col-xs-8 left-aligned paddingLeftEventTitle">Welcome to the workshop - overview & goals kit-bag pick-up
										</div>
										<div class="col-xs-2 left-aligned paddingLeftSpeaker">Ritu Arora, TACC/ Sharda Dixit, C-DAC
										</div>
								</div>
								<div class="col-xs-12"><br /></div>
								<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">
											</div>
											<div class="col-xs-8 left-aligned">Welcome from C-DAC
											</div>
											<div class="col-xs-2 left-aligned">Hemant Darbari, C-DAC
											</div>
								</div>
								<div class="col-xs-12 hideAbove500 styleFont">
										<div class="col-xs-2 paddingLeftEventTime">
										</div>
										<div class="col-xs-8 left-aligned paddingLeftEventTitle">Welcome from C-DAC
										</div>
										<div class="col-xs-2 left-aligned paddingLeftSpeaker">Hemant Darbari, C-DAC
										</div>
								</div>
								<div class="col-xs-12"><hr /></div>
								<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">10:00-10:30
											</div>
											<div class="col-xs-8 left-aligned">Advanced Cyberinfrastructure in Science and Engineering
											</div>
											<div class="col-xs-2 left-aligned">Vipin Chaudhary, NSF
											</div>
								</div>
								<div class="col-xs-12 hideAbove500 styleFont">
										<div class="col-xs-2 paddingLeftEventTime">10:00-10:30
										</div>
										<div class="col-xs-8 left-aligned paddingLeftEventTitle">Advanced Cyberinfrastructure in Science and Engineering
										</div>
										<div class="col-xs-2 left-aligned paddingLeftSpeaker">Vipin Chaudhary, NSF
										</div>
								</div>
								<div class="hideBelow760">
											<div class="col-xs-3">
													<img src="../img/Chaudhary.jpg" class="speakersPic addMarginTopPic" alt="Chaudhary Picture">
								</div>
								<div class="col-xs-9 desc">
														<p class="fontType justified">
															<span class="bold">Abstract: </span>Advanced cyberinfrastructure and the ability to perform large-scale simulations and accumulate massive amounts of data have revolutionized scientific and engineering disciplines. In this talk I will give an overview of the National Strategic Computing Initiative (NSCI) that was launched by Executive Order (EO) 13702 in July 2015 to advance U.S. leadership in high performance computing (HPC). The NSCI is a whole-of-nation effort designed to create a cohesive, multi-agency strategic vision and Federal investment strategy, executed in collaboration with industry and academia, to maximize the benefits of HPC for the United States. I will then discuss NSF’s role in NSCI and present three cross-cutting software programs ranging from extreme scale parallelism to supporting robust, reliable and sustainable software that will support and advance sustained scientific innovation and discovery.
															<br />

															<span class="bold"><br />Bio: </span>A veteran of High Performance Computing (HPC), Dr. Chaudhary has been actively participating in the science, business, government, and technology innovation frontiers of HPC for over two decades. His contributions range from heading research laboratories and holding executive management positions, to starting new technology ventures. He is currently a Program Director in the Office of Advanced Cyberinfrastructure at National Science Foundation. He is Empire Innovation Professor of Computer Science and Engineering at the Center for Computational Research at the New York State Center of Excellence in Bioinformatics and Life Sciences at SUNY Buffalo, and the Director of the university’s Data Intensive Computing Initiative. He is also the co-founder of the Center for Computational and Data-Enabled Science and Engineering.<br /><br />

														He cofounded Scalable Informatics, a leading provider of pragmatic, high performance software-defined storage and compute solutions to a wide range of markets, from financial and scientific computing to research and big data analytics. From 2010 to 2013, Dr. Chaudhary was the Chief Executive Officer of Computational Research Laboratories (CRL) where he grew the company globally to be an HPC cloud and solutions leader before selling it to Tata Consulting Services. Prior to this, as Senior Director of Advanced Development at Cradle Technologies, Inc., he was responsible for advanced programming tools for multi-processor chips. He was also the Chief Architect at Corio Inc., which had a successful IPO in June, 2000.<br /><br />

														Dr. Chaudhary’s research interests are in High Performance Computing and Applications to Science, Engineering, Biology, and Medicine; Big Data; Computer Assisted Diagnosis and Interventions; Medical Image Processing; Computer Architecture and Embedded Systems; and Spectrum Management. He has published approximately 200 papers in peer-reviewed journals and conferences and has been the principal or co-principal investigator on over $28 million in research projects from government agencies and industry. Dr. Chaudhary was awarded the prestigious President of India Gold Medal in 1986 for securing the first rank amongst graduating students at the Indian Institute of Technology (IIT). He received the B.Tech. (Hons.) degree in Computer Science and Engineering from the Indian Institute of Technology, Kharagpur, in 1986 and a Ph.D. degree from The University of Texas at Austin in 1992.</p><br />
											</div>
										</div>
										<div class="hideAbove760">
											 <div class="col-lg-3">
													 <img src="../img/Chaudhary.jpg" class="speakersPic addMarginTopPic" alt="Chaudhary Picture">
											 </div>
											 <div class="col-lg-9 desc">
														 <p class="styleFont fontType justified">
															 <span class="bold">Abstract: </span>Advanced cyberinfrastructure and the ability to perform large-scale simulations and accumulate massive amounts of data have revolutionized scientific and engineering disciplines. In this talk I will give an overview of the National Strategic Computing Initiative (NSCI) that was launched by Executive Order (EO) 13702 in July 2015 to advance U.S. leadership in high performance computing (HPC). The NSCI is a whole-of-nation effort designed to create a cohesive, multi-agency strategic vision and Federal investment strategy, executed in collaboration with industry and academia, to maximize the benefits of HPC for the United States. I will then discuss NSF’s role in NSCI and present three cross-cutting software programs ranging from extreme scale parallelism to supporting robust, reliable and sustainable software that will support and advance sustained scientific innovation and discovery.
															 <br />

															 <span class="bold"><br />Bio: </span>A veteran of High Performance Computing (HPC), Dr. Chaudhary has been actively participating in the science, business, government, and technology innovation frontiers of HPC for over two decades. His contributions range from heading research laboratories and holding executive management positions, to starting new technology ventures. He is currently a Program Director in the Office of Advanced Cyberinfrastructure at National Science Foundation. He is Empire Innovation Professor of Computer Science and Engineering at the Center for Computational Research at the New York State Center of Excellence in Bioinformatics and Life Sciences at SUNY Buffalo, and the Director of the university’s Data Intensive Computing Initiative. He is also the co-founder of the Center for Computational and Data-Enabled Science and Engineering.<br /><br />

														 He cofounded Scalable Informatics, a leading provider of pragmatic, high performance software-defined storage and compute solutions to a wide range of markets, from financial and scientific computing to research and big data analytics. From 2010 to 2013, Dr. Chaudhary was the Chief Executive Officer of Computational Research Laboratories (CRL) where he grew the company globally to be an HPC cloud and solutions leader before selling it to Tata Consulting Services. Prior to this, as Senior Director of Advanced Development at Cradle Technologies, Inc., he was responsible for advanced programming tools for multi-processor chips. He was also the Chief Architect at Corio Inc., which had a successful IPO in June, 2000.<br /><br />

														 Dr. Chaudhary’s research interests are in High Performance Computing and Applications to Science, Engineering, Biology, and Medicine; Big Data; Computer Assisted Diagnosis and Interventions; Medical Image Processing; Computer Architecture and Embedded Systems; and Spectrum Management. He has published approximately 200 papers in peer-reviewed journals and conferences and has been the principal or co-principal investigator on over $28 million in research projects from government agencies and industry. Dr. Chaudhary was awarded the prestigious President of India Gold Medal in 1986 for securing the first rank amongst graduating students at the Indian Institute of Technology (IIT). He received the B.Tech. (Hons.) degree in Computer Science and Engineering from the Indian Institute of Technology, Kharagpur, in 1986 and a Ph.D. degree from The University of Texas at Austin in 1992.</p><br />
											 </div>
										 </div>
									<div class="col-xs-12"><hr /></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">10:30-11:00
											</div>
											<div class="col-xs-8 left-aligned">A User-Defined Code Transformation Approach to Separation of Performance Concerns
											</div>
											<div class="col-xs-2 left-aligned">Hiroyuki Takizawa, Tohoku University
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">10:30-11:00
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">A User-Defined Code Transformation Approach to Separation of Performance Concerns
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">Hiroyuki Takizawa, Tohoku University
											</div>
									</div>
									<div class="hideBelow760">
												<div class="col-xs-3">
														<br />
														<img src="../img/Takizawa.jpg" class="speakersPic" alt="Takizawa Picture">
									</div>
									<div class="col-xs-9 desc">
															<p class="fontType justified">

																<span class="bold">Abstract: </span>Today, high-performance computing (HPC) application codes are often optimized and specialized for a particular system configuration to exploit the system's potential. One severe problem is that simply modifying an HPC application code often results in degrading the performance portability, readability, and maintainability of the code. Therefore, we have been developing a code transformation framework, Xevolver, so that users can easily define their own code transformation	rules for individual cases, in order to express how each application	code should be changed to achieve high performance. In this talk, I will briefly review the Xevolver framework and introduce some case studies to discuss the benefits of the user-defined code transformation approach.<br />

																<span class="bold"><br />Bio: </span>Hiroyuki Takizawa is currently a professor of Cyberscience Center, Tohoku University. His research interests include performance-aware programming, high-performance computing systems and their applications. Since 2011, he is leading a research project, supported by JST CREST, to explore an effective way of assisting legacy HPC code migration to future-generation extreme-scale computing systems. He received the B.E. Degree in Mechanical Engineering, and the M.S. and Ph.D. Degrees in Information Sciences from Tohoku University in 1995, 1997 and 1999, respectively.
															</p>
												</div>
											</div>
											<div class="hideAbove760">
												<div class="col-lg-3">
														<img src="../img/Takizawa.jpg" class="speakersPic addMarginTopPic" alt="Takizawa Picture">
												</div>
												<div class="col-lg-9 speakerDesc">
															<p class="styleFont fontType justified">
																<span class="bold">Abstract: </span>Today, high-performance computing (HPC) application codes are often optimized and specialized for a particular system configuration to exploit the system's potential. One severe problem is that simply modifying an HPC application code often results in degrading the performance portability, readability, and maintainability of the code. Therefore, we have been developing a code transformation framework, Xevolver, so that users can easily define their own code transformation	rules for individual cases, in order to express how each application	code should be changed to achieve high performance. In this talk, I will briefly review the Xevolver framework and introduce some case studies to discuss the benefits of the user-defined code transformation approach.<br />

																<span class="bold"><br />Bio: </span>Hiroyuki Takizawa is currently a professor of Cyberscience Center, Tohoku University. His research interests include performance-aware programming, high-performance computing systems and their applications. Since 2011, he is leading a research project, supported by JST CREST, to explore an effective way of assisting legacy HPC code migration to future-generation extreme-scale computing systems. He received the B.E. Degree in Mechanical Engineering, and the M.S. and Ph.D. Degrees in Information Sciences from Tohoku University in 1995, 1997 and 1999, respectively.
															</p>
												</div>
											</div>

									<div class="col-xs-12"><hr class="rmBottomMargin"/></div>
									<div class="col-xs-12 hideBelow500 redBackground">
											<div class="col-xs-2">11:00-11:15
											</div>
											<div class="col-xs-8 center-aligned">COFFEE BREAK & NETWORKING
											</div>
											<div class="col-xs-2 left-aligned">
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont redBackground">
											<div class="col-xs-2 paddingLeftEventTime">11:00-11:15
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">COFFEE BREAK & NETWORKING
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">
											</div>
									</div>
									<div class="col-xs-12"><hr class="rmTopMargin"/></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">11:15-11:45
											</div>
											<div class="col-xs-8 left-aligned">Technologies for Exascale Computing
											</div>
											<div class="col-xs-2 left-aligned">Nash Palaniswamy, Intel
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">11:15-11:45
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">Technologies for Exascale Computing
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">Nash Palaniswamy, Intel
											</div>
									</div>
										<div class="hideBelow760">
											<div class="col-xs-3">
													<img src="../img/Avinash.jpg" class="avinashPic addMarginTopPic" alt="Dr. Nash Palaniswamy's Picture">
											</div>
											<div class="col-xs-9 desc">
													<p class="fontType justified">
														<span class="bold">Abstract: </span>Intel is investing in a broad set of technologies to move computing to the address the challenges of Exascale computing. These technologies are targeted to reach next generation performance in a configurable system that can achieve exceptional performance in data analytics, traditional high performance computing and artificial intelligence. Being able to address all of these application domains is of critical importance.<br /><br />

														Intel is investing in processors, fabric, memory and software.  Each will be discussed along with their respective importance in achieving Exascale.<br />

													<span class="bold"><br />Bio: </span>Dr. Nash Palaniswamy has been at Intel since October 2005, and focuses in the area of Enterprise and High Performance Computing in the Datacenter group. He is currently the Senior Director for Worldwide Solutions Enablement and Revenue Management for Enterprise and HPC.  In this role, he is responsible for managing all strategic opportunities in Enterprise and HPC and managing and meeting revenue for the Enterprise and Government segment in Intel’s datacenter group. Dr. Palaniswamy leads a team that drives strategic opportunities worldwide (solutions, architecture, products, business frameworks, etc) in collaboration with Intel’s ecosystem partners.<br /><br />

													His prior responsibilities at Intel included being the lead for worldwide business development and operations for Intel® Technical Computing Solutions, Intel® QuickAssist Technology based accelerators in HPC, and World Wide Web Consortium Advisory Committee representative from Intel. Prior to joining Intel as part of the acquisition of Conformative Systems, an XML Accelerator Company, he has served in several senior executive positions in the industry including being the Director of System Architecture at Conformative Systems, CTO/VP of Engineering at MSU Devices (a publicly traded company), and Director of Java Program Office and Wireless Software Strategy in the Digital Experience Group of Motorola, Inc.<br /><br />

													Dr. Palaniswamy holds a B.S. in Electronics and Communications Engineering from Anna University (Chennai, India) and an M.S. and Ph.D. from the University of Cincinnati in Electrical and Computer Engineering.
													</p>
											</div>
										</div>
										<div class="hideAbove760">
											<div class="col-lg-3">
													<img src="../img/Avinash.jpg" class="avinashPic addMarginTopPic" alt="Dr. Nash Palaniswamy's Picture">
											</div>
											<div class="col-lg-9 desc">
													<p class="styleFont fontType justified">
														<span class="bold">Abstract: </span>Intel is investing in a broad set of technologies to move computing to the address the challenges of Exascale computing. These technologies are targeted to reach next generation performance in a configurable system that can achieve exceptional performance in data analytics, traditional high performance computing and artificial intelligence. Being able to address all of these application domains is of critical importance.<br /><br />

														Intel is investing in processors, fabric, memory and software.  Each will be discussed along with their respective importance in achieving Exascale.<br />

													<span class="bold"><br />Bio: </span>Dr. Nash Palaniswamy has been at Intel since October 2005, and focuses in the area of Enterprise and High Performance Computing in the Datacenter group. He is currently the Senior Director for Worldwide Solutions Enablement and Revenue Management for Enterprise and HPC.  In this role, he is responsible for managing all strategic opportunities in Enterprise and HPC and managing and meeting revenue for the Enterprise and Government segment in Intel’s datacenter group. Dr. Palaniswamy leads a team that drives strategic opportunities worldwide (solutions, architecture, products, business frameworks, etc) in collaboration with Intel’s ecosystem partners.<br /><br />

													His prior responsibilities at Intel included being the lead for worldwide business development and operations for Intel® Technical Computing Solutions, Intel® QuickAssist Technology based accelerators in HPC, and World Wide Web Consortium Advisory Committee representative from Intel. Prior to joining Intel as part of the acquisition of Conformative Systems, an XML Accelerator Company, he has served in several senior executive positions in the industry including being the Director of System Architecture at Conformative Systems, CTO/VP of Engineering at MSU Devices (a publicly traded company), and Director of Java Program Office and Wireless Software Strategy in the Digital Experience Group of Motorola, Inc.<br /><br />

													Dr. Palaniswamy holds a B.S. in Electronics and Communications Engineering from Anna University (Chennai, India) and an M.S. and Ph.D. from the University of Cincinnati in Electrical and Computer Engineering.
													</p>
											</div>
										</div>
									<div class="col-xs-12"><hr /></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">11:45-12:15
											</div>
											<div class="col-xs-8 left-aligned">Overcoming Deployment and Configuration Challenges in High Performance Computing via Model-driven Engineering Technologies
											</div>
											<div class="col-xs-2 left-aligned">Aniruddha Gokhale, Vanderbilt University
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">11:45-12:15
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">Overcoming Deployment and Configuration Challenges in High Performance Computing via Model-driven Engineering Technologies
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">Aniruddha Gokhale, Vanderbilt University
											</div>
									</div>
									<div class="hideBelow760">
											<div class="col-xs-3">
													<img src="../img/Aniruddha.jpg" class="aniruddhaPic addMarginTopPic" alt="Aniruddha Picture">
											</div>
											<div class="col-xs-9 desc">
													<p class="fontType justified">
														<span class="bold">Abstract: </span>As systems scale in size and complexity, reasoning about their properties and controlling their behavior requires complex simulations, which often involves multiple interacting co-simulators that must be deployed and configured on high performance computing resources. Increasingly, cloud platforms which may even be federated, offer cost-effective solutions to realize such deployments. However, researchers and practitioners alike often face a plethora of challenges stemming from the need for rapid provisioning/deprovisioning, ensuring reliability, defining strategies for autoscaling against changing workloads, handling resource unavailabilities, and exploiting modern features such as GPUs, FPGAs, and NUMA architectures to name a few, for which they generally tend to lack the expertise to overcome these challenges. Model-driven engineering (MDE) offers significant promise to address these challenges by providing the users with intuitive abstractions and automating the deployment and configuration tasks. This talk describes our ongoing work in this space and will highlight both the MDE and systems solutions that we are investigating.<br />

														<span class="bold"><br />Bio: </span>Dr. Aniruddha S. Gokhale is an Associate Professor in the Department of Electrical Engineering and Computer Science, and Senior Research Scientist at the Institute for Software Integrated Systems (ISIS) both at Vanderbilt University, Nashville, TN, USA. His current research focuses on developing novel solutions to emerging challenges in edge-to-cloud computing, real-time stream processing, and publish/subscribe systems as applied to cyber physical systems including smart transportation and smart cities. He is also working on using cloud computing technologies for STEM education. Dr. Gokhale obtained his B.E (Computer Engineering) from University of Pune, India, 1989; MS (Computer Science) from Arizona State University, 1992; and D.Sc (Computer Science) from Washington University in St. Louis, 1998. Prior to joining Vanderbilt, Dr. Gokhale was a member of technical staff at Lucent Bell Laboratories, NJ. Dr. Gokhale is a Senior member of both IEEE and ACM, and a member of ASEE. His research has been funded over the years by DARPA, DoD, industry and NSF including a NSF CAREER award in 2009.<br />
													</p>
											</div>
										</div>
										<div class="hideAbove760">
												<div class="col-lg-3">
														<img src="../img/Aniruddha.jpg" class="aniruddhaPic addMarginTopPic" alt="Aniruddha Picture">
												</div>
												<div class="col-lg-9 desc">
														<p class="styleFont fontType justified">
															<span class="bold">Abstract: </span>As systems scale in size and complexity, reasoning about their properties and controlling their behavior requires complex simulations, which often involves multiple interacting co-simulators that must be deployed and configured on high performance computing resources. Increasingly, cloud platforms which may even be federated, offer cost-effective solutions to realize such deployments. However, researchers and practitioners alike often face a plethora of challenges stemming from the need for rapid provisioning/deprovisioning, ensuring reliability, defining strategies for autoscaling against changing workloads, handling resource unavailabilities, and exploiting modern features such as GPUs, FPGAs, and NUMA architectures to name a few, for which they generally tend to lack the expertise to overcome these challenges. Model-driven engineering (MDE) offers significant promise to address these challenges by providing the users with intuitive abstractions and automating the deployment and configuration tasks. This talk describes our ongoing work in this space and will highlight both the MDE and systems solutions that we are investigating.<br />

															<span class="bold"><br />Bio: </span>Dr. Aniruddha S. Gokhale is an Associate Professor in the Department of Electrical Engineering and Computer Science, and Senior Research Scientist at the Institute for Software Integrated Systems (ISIS) both at Vanderbilt University, Nashville, TN, USA. His current research focuses on developing novel solutions to emerging challenges in edge-to-cloud computing, real-time stream processing, and publish/subscribe systems as applied to cyber physical systems including smart transportation and smart cities. He is also working on using cloud computing technologies for STEM education. Dr. Gokhale obtained his B.E (Computer Engineering) from University of Pune, India, 1989; MS (Computer Science) from Arizona State University, 1992; and D.Sc (Computer Science) from Washington University in St. Louis, 1998. Prior to joining Vanderbilt, Dr. Gokhale was a member of technical staff at Lucent Bell Laboratories, NJ. Dr. Gokhale is a Senior member of both IEEE and ACM, and a member of ASEE. His research has been funded over the years by DARPA, DoD, industry and NSF including a NSF CAREER award in 2009.<br />
														</p>
												</div>
											</div>
									<div class="col-xs-12"><hr /></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">12:15-12:45
											</div>
											<div class="col-xs-8 left-aligned">The EX Factor in the Exascale Era: Factors driving changes in HPC
											</div>
											<div class="col-xs-2 left-aligned">Bharatkumar Sharma, Nvidia
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">12:15-12:45
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">The EX Factor in the Exascale Era: Factors driving changes in HPC
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">Bharatkumar Sharma, Nvidia
											</div>
									</div>
									<div class="hideBelow760">
											<div class="col-xs-3">
													<img src="../img/Bharatkumar.jpg" class="speakersPic addMarginTopPic" alt="Takizawa Picture">
											</div>
									<div class="col-xs-9 desc">
													<p class="fontType justified">
														<span class="bold">Abstract: </span>GPU’s has been used to accelerate HPC algorithms which are based on first principles theory and are proven statistical models for accurate results in multiple science domains. This talk will provide insights into the HPC domain and how it affects the programs you write today and in the future in various domains.<br />

														<span class="bold"><br />Bio: </span>Bharatkumar Sharma obtained master degree in Information Technology from Indian Institute of Information Technology, Bangalore. He has around 10 years of development and research experience in domain of Software Architecture, Distributed and Parallel Computing. He is currently working with Nvidia as a Senior Solution Architect, South Asia. He has published papers and journal articles in field of Parallel Computing and Software Architecture.<br />
													</p>
											</div>
										</div>
										<div class="hideAbove760">
											<div class="col-lg-3">
													<img src="../img/Bharatkumar.jpg" class="speakersPic addMarginTopPic" alt="Takizawa Picture">
											</div>
											<div class="col-lg-9 desc">
													<p class="styleFont fontType justified">
														<span class="bold">Abstract: </span>GPU’s has been used to accelerate HPC algorithms which are based on first principles theory and are proven statistical models for accurate results in multiple science domains. This talk will provide insights into the HPC domain and how it affects the programs you write today and in the future in various domains.<br />

														<span class="bold"><br />Bio: </span>Bharatkumar Sharma obtained master degree in Information Technology from Indian Institute of Information Technology, Bangalore. He has around 10 years of development and research experience in domain of Software Architecture, Distributed and Parallel Computing. He is currently working with Nvidia as a Senior Solution Architect, South Asia. He has published papers and journal articles in field of Parallel Computing and Software Architecture.<br />
													</p>
											</div>
										</div>
									<div class="col-xs-12"><hr class="rmBottomMargin"/></div>
									<div class="col-xs-12 hideBelow500 redBackground">
											<div class="col-md-2">12:45-13:45
											</div>
											<div class="col-md-8 center-aligned">LUNCH BREAK & NETWORKING
											</div>
											<div class="col-md-2 left-aligned">
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 redBackground styleFont">
											<div class="col-xs-2 paddingLeftEventTime">12:45-13:45
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">LUNCH BREAK & NETWORKING
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">
											</div>
									</div>
									<div class="col-xs-12"><hr class="rmTopMargin"/></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">13:45-14:50
											</div>
											<div class="col-xs-8 left-aligned"><ul class="noList">
												                            <li>Neethi Suresh, Lois Thomas and Bipin Kumar, <i>"DNS for large domains: Challenges for computation and storage" </i></li>
															    <li><br></li><li>Manu Awasthi, <i>"DRAM Organization Aware OS Page Allocation"</i></li>
																	<li><hr class="rmBottomMargin"/></li>

																	<li><span class="bold"><br />Abstract: </span>The applications in HPC and datacenter domains are beginning to have increasingly larger memory footprints running into multiple tens of gigabytes. Contemporary HPC server also have multiple NUMA nodes and the NUMA factor is increasing at an alarming pace with the increase in number of NUMA nodes. As a result, the average cost of main memory access has also been increasing. Furthermore, for a typical HPC application, DRAM access latencies are a function of 1) the memory allocation policies of OS (across NUMA nodes) and 2) data layout in DRAM DIMMs (per-NUMA node).<br /><br />

																	A DRAM DIMM has a hierarchical structure where it is divided into ranks, banks, rows and columns. There are multiple such DIMMs on each channel and there could be multiple channels emanating from one memory controller. Furthermore, there could be multiple memory controllers per NUMA node. Data layout across DRAM DIMMs and channels is a function of memory controller specific policies like address mapping schemes, which potentially can differ between multiple controllers on the same node.<br /><br />

																	The OS allocates memory at the granularity of pages. Since the OS has no knowledge of the underlying architecture and organization of per-NUMA node DRAM, each OS page could potentially be concentrated to a subset of available channels, DIMMs, ranks, banks and arrays, putting them under pressure and reducing performance. In order to get most benefit out of existing architectures, it is necessary that the OS allocate memory (at page granularity) while being cognizant of the inner architecture and organization of per-NUMA node DRAM.<br /><br />

																	In this proposal, we propose mechanisms where the OS, in addition to reducing memory allocation for processes across NUMA nodes, also takes into account the organization of DRAM DIMMs (channels, ranks, banks, rows, columns etc.) for every memory controller. This will help the OS make page allocation decisions which maximize the use of available parallelism, increase locality and hence reduce overall memory access latency, decreasing application runtimes.</li>
																	<li><span class="bold"><br />Presentation Slides: </span><br />
																	<a href="../slides/SCEC2017Final.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation">DRAM Organization Aware OS Page Allocation</a></li>
																	<li><hr class="rmBottomMargin"/></li>
															    <li><br></li><li>Gouri Kadam and Shweta Nayak, <i>"Implementation of OpenSource Structural Engineering Application OpenSees on GPU platform"</i></li>
																	<li><hr class="rmBottomMargin"/></li>

																	<li><span class="bold"><br />Abstract: </span>GPU computing is emerging as an alternative to CPUs for throughput oriented applications because of their number of cores embedded on the same chip and speed-up in computing. GPUs provide general purpose computing using dedicated libraries such as CUDA (for NVIDIA cards, based on a SIMD architecture that makes it possible to handle a very large number of hardware threads concurrently, and can be used for various purposes. This paper explains customization of a well-known open source earthquake engineering application OpenSees on hybrid platform using GPU enabled open source libraries.<br /><br />

																	Available GPU enabled version of OpenSees provided by Xinzheng Lu, Linlin Xie (Tsinghua University, China) uses CulaS4 and CulaS5, which use the Cula library and currently supported on window’s platform. Both of these limits the usage of OpenSees as an open source platform. To overcome this, we have modified existing CuSPSolver to use only freely available CUSP library. Speed up improvement is achieved by diverting analysis component to GPU architecture<br /><br />

																	GPU enablement will help researchers and scientists to use this open-source platform for their research in structural and earthquake engineering domain using OpenSees. Also as there are minor changes required to be done in the input script and no great programming efforts required to run it using GPU enabled OpenSees, these modifications are very user friendly. Speedup around 2.14x is observed when tested with some examples. Different types of examples were studied to compare the results and it has been observed that performance will increases with increase in complexity of the problem. Nodal displacement results of one of the example were compared with CPU and GPU simulations which were matching. This validates the methodology used for GPU enablement. These modifications were also integrated in the mainstream of OpenSees code at Berkley and now available to all structural engineering community. All the study carried out is for single CPU and single GPU. There is lot of scope to extend this work for multi GPU, using other accelerators and using OpenCL.</li>
																	<li><span class="bold"><br />Presentation Slides: </span><br />
																	<a href="../slides/SCEC2017_GPU_OpenSees.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation">Implementation of OpenSource Structural Engineering Application OpenSees on GPU platform</a></li>
																	<li><hr class="rmBottomMargin"/></li>
															    <li><br></li><li>Mangala N, Deepika H.V, Prachi Pandey and Shamjith K V, <i>"Adaptive Resource Allocation Technique for Exascale Systems"</i></li>
															    <li><br></li><li>Shreya Bokare, Sanjay Pawar and Veena Tyagi.<i>"Network coded Storage I/O subsystem for HPC exascale applications"</i></li>
																	<li><hr class="rmBottomMargin"/></li>
																	<li><span class="bold"><br />Abstract: </span>Tremendous data growth in exascale HPC applications will generate variety of
datasets. This will impose two important challenges on the storage I/O system i) efficiently
process large data sets, ii) improved fault tolerance. The exascale storage systems are
expected to be capable enough providing bandwidth guarantee and applying coding scheme
optimized for the type of data set.<br /><br />
Traditional RAID architecture in HPC storage system is being replaced with network coding
techniques to provide optimal fault tolerance for data. There exists variation in network
codes based on traditional erasure (reed-Solomon) with number of libraries
implementations, which notably differ in terms of complexity and implementation for
specific optimizations.<br /><br />
We propose an I/O subsystem to provide bandwidth guarantee along with optimized fault
tolerance for variety of HPC/big datasets. The proposed storage I/O subsystem consists of
software defined storage gateways and controller working along with filesystem MDS. We
propose an I/O caching mechanism with hierarchical storage structure embedded at storage
gateway. The designed caching policy will cache smaller files on faster storage drives (like
SSD’s) eliminating long access latency for smaller sized files. The storage gateways are
running with systematic erasure (RS) code, with preconfigured encoding-decoding
algorithms. At each gateway, network coding (erasure coding encode-decode) is performed
at multiple tiers with specified parameter. Geometric programming is used to calculate
parameters at each tier based on data demand. The proposed system provides bandwidth
guarantee with caching mechanism at hierarchical storage layers. Flexible erasure coding
provides optimized fault tolerance for variety of application datasets.<br /><br />
Future work will focus on design of flexible caching and replacement policy for real time
workload and an adaptive encode-decode algorithm for highly skewed data demand.
																	</li>
																	<li><span class="bold"><br />Presentation Slides: </span><a href="../slides/networkCodedStorageIO.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation"><br/>NETWORK CODED STORAGE I/O SUBSYSTEM FOR HPC EXASCALE APPLICATIONS</a></li>
																	<li><hr class="rmBottomMargin"/></li>
															    <li><br></li><li>Manavalan R.<i>"Application level challenges and issues of processing different frequency, polarization and incidence angle Synthetic Aperture Radar data using distributed computing resources"</i></li>
																	<li><hr class="rmBottomMargin"/></li>
																	<li><span class="bold"><br />Abstract:</span>The role and need of Synthetic Aperture Radar (SAR) technology in various geospatial applications is proved beyond the doubts as SAR can make available critical information about on the filed information with centimeter to millimeter accuracy. The science of processing such SAR data using the distributed computing resources is more than two decade old and as of now been broadly bolstered by worldwide HPC labs for example, ESA’s G-POD (Grid Processing on Demand), Peppers (A Performance Emphasized Production Environment for Remote Sensing), DLR distributed SAR data processing environment, Center for Earth Observation and Digital Earth (CEODE) of Chinese Academy of Sciences (CAS),..etc. This paper discusses the difficulties and issues of Geospatial application users who are working with multiple sets of voluminous temporal SAR data and brings out the importance of developing and deploying all-purpose HPC based infrastructure environment that can meet the expectations of application users. Specific situations which require simultaneous processing of different frequency, polarization and incidence angle SAR data using the distributed HPC resources and related requirements will be discussed. For example during the SAR raw data processing, the need and importance of simultaneous processing and extraction of different polarization images as well as in completing its related post processing tasks and mapping operations will be highlighted. When the same has to be done at a regional scale, mainly to simulate large scale disaster events with various multi-look factors as well as with different post processing filtering options the need of developing and deploying such real time solutions on an exascale computing environment can be well understood. As on date such large scale complex SAR application model supporting the above mentioned expectations is yet to be developed. In line to this this talk will brings out the importance of prototyping such HPC based SAR data processing environment which can support the complete application cycle of real time regional disaster management simulations and its related operations. Any such operational setup certainly needs effective coordination of worldwide space agencies as well as distributed HPC labs.</li>
																	<li><span class="bold"><br />Presentation Slides: </span><br />
																	<a href="../slides/13DEC2017-CDACB-Manavaln-Slides.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation">‘Application level’ challenges and issues of processing different frequency, polarization and incidence angle Synthetic Aperture Radar data using distributed computing resources</a><br />
																	</li>
																	<li><hr class="rmBottomMargin"/></li>
												                            <li><br></li><li>Venkatesh Shenoi, Janakiraman S and Sandeep Joshi, <i> "Towards energy efficient numerical weather prediction Scalable algorithms and approaches"</i></li>
																										<li><hr class="rmBottomMargin"/></li>
																										<li><span class="bold"><br />Abstract:</span>The weather forecasting has a major impact on the society. The meteorologists have been using the
numerical weather prediction models for the operation forecast for over several decades ever since
Richardson's attempt towards numerical models for weather prediction. The weather codes have
consumed a good portion of the computational power available in several supercomputing centers.
This has led to the need to improve the computational efficiency of the codes/models to achieve
better resolution moving towards the better accuracy of the forecasts. However, these large scale
computations are possible only at the expense of the huge energy budget due to the increasing
requirement of computational resources as well as the cooling infrastructure required to maintain
them. But with the energy budget fixed at 20 MW it is even more challenging as we are compelled
to move towards energy efficiency of the weather codes. We focus towards the spectral transform
method as case study for this talk. This method has been in use in NCAR and ECMWF for the
operational forecasts.<br /><br />

In this talk, we shall discuss the basics of numerical weather predictions and move towards the test
bed shallow water model to be solved by spectral transform method. The highlights of the approach
with regard to algorithm, its scalability and improvements leading to the reduction in the
computational complexity will be discussed. To conclude, some of the recent efforts on scaling up
the solver for shallow water equations are discussed along with the glimpse of the ongoing efforts in
the ESCAPE project. This talk is inspired by our project proposal on “Scalable algorithmic
approach to spectral transform method” to be pursued under NSM, India.</li>
																									       <li><span class="bold"><br />Presentation Slides: </span><br />
																								                <a href="../slides/venkatesh_shenoi_slides.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation">Towards Energy Efficient Numerical Weather Prediction</a><br />
																										</li>
																										<li><hr class="rmBottomMargin"/></li>
															    <li><br></li></ul>
											</div>
											<div class="col-xs-2 left-aligned">TBD
											</div>
									</div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">14:50-15:10
											</div>
											<div class="col-xs-8 left-aligned">Narendra Karmarkar, VCV Rao, <i>"MPPLAB(E-Teacher)"</i>
											</div>
											<div class="col-xs-2 left-aligned">
											</div>
									</div>
									<div class="hideBelow760">
											<div class="col-xs-3">
													<img src="../img/vcv.jpg" class="speakersPic addMarginTopPic" alt="VCV Rao Picture">
											</div>
									<div class="col-xs-9 desc">
													<p class="fontType justified">
														<span class="bold">Abstract: </span>HPC has huge future scope for scientific applications, large data bases, AI/deep learning, business applications in financial industry, telecom (particularly 5G) etc. Unfortunately, present systems are serving the market with separate products, in a rather fragmented manner. Challenge is to create a "Unified Architecture" that will unify the user space, following a "Top-down" approach instead of “Bottom-up" approach which forces applications to try to fit their code to peculiarities of particular cpu's, accelerators, system architecture etc. Today's potential of HPC and silicon technology is grossly underutilized due to effort and time spent in tailoring parallel code to specific machines.    Incorporation of FPGA-based reconfigurability in general applications is getting delayed unnecessarily. If economic benefits based on what is eminently feasible technologically, are not delivered to the society quickly enough, it slows down investments in further technological enhancements. All HPC users will benefit from this in the long run. At the same time, we are focusing on how very complex code can be put  together in shorter time-span, and in such a way that investment in top-level code design is long-lived in face of  anticipated changes in successive generations of chips, interfaces etc. Due to vast scope, we will present only a broad overview and elaborate only on couple of aspects.  Improving programmer productivity by designing and writing parallel code at multiple levels of abstraction by providing more expressive notations, tools for transforming one level to the next is required. It is also necessary to do away with artificial boundary between hardware description languages and how far traditional compilers reach starting from high level languages. This will ensure more seamless utilization of FPGA-based re-configurability in the unified system architecture. Since hardware aspects are too complex, only one particular aspect related to GPU's will be covered.  Initially, we’ll be addressing application of optimization algorithms to economic modelling and telecom systems.<br />

														<span class="bold"><br />Bio: </span>VCV.Rao received his Master degree in Mathematics from Andhra University, India in 1985 and Ph.D degree in Mathematics from IIT-Kanpur in the year 1993. He is associated with C-DAC since 1993 on High Performance Computing projects. He contributed to design, develop and deploy  C-DAC’s PARAM Series of Supercomputers, GARUDA Grid Computing project, Parallel Computing workshops, contributed to PARAM series at premier academic Institutions. Currently, he is an Associate Director in the High-Performance Computing Technologies (HPC-Tech) Group, at C-DAC, Pune.<br />
													</p>
											</div>
										</div>
										<div class="hideBelow760">
												<div class="col-xs-3">
														<img src="../img/Narendra.jpg" class="speakersPic addMarginTopPic" alt="VCV Rao Picture">
												</div>
												<div class="col-xs-9 desc">
														<p class="fontType justified">
															<span class="bold"><br />Bio: </span>Karmarkar received his B.Tech in EE from IIT Bombay in 1978, M.S. from the California Institute of Technology in 1979 and Ph.D. in Computer Science from the University of California, Berkeley in 1983. He is well known for linear programming algorithms - a cornerstone in the field of Linear Programming. He is a Fellow of Bell Laboratories (1987 onwards). In 2006-2007, he served as scientific advisor to the Chairman Tata group, founded CRL and architected "EKA" system, which stands for "Embedded Karmarkar Algorithm”. Currently, he is a Consultant Chief Architect in C-DAC, Pune. He is also a distinguished visiting professor at several institutes such as IISc, and IITs.<br />
														</p>
												</div>
											</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">13:45-14:50
											</div>
											<div class="col-xs-8 left-aligned"><ul class="noList">
																										<li>Neethi Suresh, Lois Thomas and Bipin Kumar, <i>"DNS for large domains: Challenges for computation and storage" </i></li>
																	<li><br></li><li>Manu Awasthi, <i>"DRAM Organization Aware OS Page Allocation"</i></li>
																	<li><hr class="rmBottomMargin"/></li>

																	<li><span class="bold"><br />Abstract: </span>The applications in HPC and datacenter domains are beginning to have increasingly larger memory footprints running into multiple tens of gigabytes. Contemporary HPC server also have multiple NUMA nodes and the NUMA factor is increasing at an alarming pace with the increase in number of NUMA nodes. As a result, the average cost of main memory access has also been increasing. Furthermore, for a typical HPC application, DRAM access latencies are a function of 1) the memory allocation policies of OS (across NUMA nodes) and 2) data layout in DRAM DIMMs (per-NUMA node).<br /><br />

																	A DRAM DIMM has a hierarchical structure where it is divided into ranks, banks, rows and columns. There are multiple such DIMMs on each channel and there could be multiple channels emanating from one memory controller. Furthermore, there could be multiple memory controllers per NUMA node. Data layout across DRAM DIMMs and channels is a function of memory controller specific policies like address mapping schemes, which potentially can differ between multiple controllers on the same node.<br /><br />

																	The OS allocates memory at the granularity of pages. Since the OS has no knowledge of the underlying architecture and organization of per-NUMA node DRAM, each OS page could potentially be concentrated to a subset of available channels, DIMMs, ranks, banks and arrays, putting them under pressure and reducing performance. In order to get most benefit out of existing architectures, it is necessary that the OS allocate memory (at page granularity) while being cognizant of the inner architecture and organization of per-NUMA node DRAM.<br /><br />

																	In this proposal, we propose mechanisms where the OS, in addition to reducing memory allocation for processes across NUMA nodes, also takes into account the organization of DRAM DIMMs (channels, ranks, banks, rows, columns etc.) for every memory controller. This will help the OS make page allocation decisions which maximize the use of available parallelism, increase locality and hence reduce overall memory access latency, decreasing application runtimes.</li>
																	<li><span class="bold"><br />Presentation Slides: </span><br />
																	<a href="../slides/SCEC2017Final.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation">DRAM Organization Aware OS Page Allocation</a></li>
																	<li><hr class="rmBottomMargin"/></li>
																	<li><br></li><li>Gouri Kadam and Shweta Nayak, <i>"Implementation of OpenSource Structural Engineering Application OpenSees on GPU platform"</i></li>
																	<li><hr class="rmBottomMargin"/></li>

																	<li><span class="bold"><br />Abstract: </span>GPU computing is emerging as an alternative to CPUs for throughput oriented applications because of their number of cores embedded on the same chip and speed-up in computing. GPUs provide general purpose computing using dedicated libraries such as CUDA (for NVIDIA cards, based on a SIMD architecture that makes it possible to handle a very large number of hardware threads concurrently, and can be used for various purposes. This paper explains customization of a well-known open source earthquake engineering application OpenSees on hybrid platform using GPU enabled open source libraries.<br /><br />

																	Available GPU enabled version of OpenSees provided by Xinzheng Lu, Linlin Xie (Tsinghua University, China) uses CulaS4 and CulaS5, which use the Cula library and currently supported on window’s platform. Both of these limits the usage of OpenSees as an open source platform. To overcome this, we have modified existing CuSPSolver to use only freely available CUSP library. Speed up improvement is achieved by diverting analysis component to GPU architecture<br /><br />

																	GPU enablement will help researchers and scientists to use this open-source platform for their research in structural and earthquake engineering domain using OpenSees. Also as there are minor changes required to be done in the input script and no great programming efforts required to run it using GPU enabled OpenSees, these modifications are very user friendly. Speedup around 2.14x is observed when tested with some examples. Different types of examples were studied to compare the results and it has been observed that performance will increases with increase in complexity of the problem. Nodal displacement results of one of the example were compared with CPU and GPU simulations which were matching. This validates the methodology used for GPU enablement. These modifications were also integrated in the mainstream of OpenSees code at Berkley and now available to all structural engineering community. All the study carried out is for single CPU and single GPU. There is lot of scope to extend this work for multi GPU, using other accelerators and using OpenCL.</li>
																	<li><span class="bold"><br />Presentation Slides: </span><br />
																	<a href="../slides/SCEC2017_GPU_OpenSees.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation">Implementation of OpenSource Structural Engineering Application OpenSees on GPU platform</a></li>
																	<li><hr class="rmBottomMargin"/></li>
																	<li><br></li><li>Mangala N, Deepika H.V, Prachi Pandey and Shamjith K V, <i>"Adaptive Resource Allocation Technique for Exascale Systems"</i></li>
																	<li><br></li><li>Shreya Bokare, Sanjay Pawar and Veena Tyagi.<i>"Network coded Storage I/O subsystem for HPC exascale applications"</i></li>
																	<li><hr class="rmBottomMargin"/></li>
																	<li class><span class="bold"><br />Abstract: </span>Tremendous data growth in exascale HPC applications will generate variety of
datasets. This will impose two important challenges on the storage I/O system i) efficiently
process large data sets, ii) improved fault tolerance. The exascale storage systems are
expected to be capable enough providing bandwidth guarantee and applying coding scheme
optimized for the type of data set.<br /><br />
Traditional RAID architecture in HPC storage system is being replaced with network coding
techniques to provide optimal fault tolerance for data. There exists variation in network
codes based on traditional erasure (reed-Solomon) with number of libraries
implementations, which notably differ in terms of complexity and implementation for
specific optimizations.<br /><br />
We propose an I/O subsystem to provide bandwidth guarantee along with optimized fault
tolerance for variety of HPC/big datasets. The proposed storage I/O subsystem consists of
software defined storage gateways and controller working along with filesystem MDS. We
propose an I/O caching mechanism with hierarchical storage structure embedded at storage
gateway. The designed caching policy will cache smaller files on faster storage drives (like
SSD’s) eliminating long access latency for smaller sized files. The storage gateways are
running with systematic erasure (RS) code, with preconfigured encoding-decoding
algorithms. At each gateway, network coding (erasure coding encode-decode) is performed
at multiple tiers with specified parameter. Geometric programming is used to calculate
parameters at each tier based on data demand. The proposed system provides bandwidth
guarantee with caching mechanism at hierarchical storage layers. Flexible erasure coding
provides optimized fault tolerance for variety of application datasets.<br /><br />
Future work will focus on design of flexible caching and replacement policy for real time
workload and an adaptive encode-decode algorithm for highly skewed data demand.
																	</li>
																	<span class="bold"><br />Presentation Slides: </span><a href="../slides/networkCodedStorageIO.pdf" id="presentation1" class="presentationSlides" target="_blank" title="PDF Presentation">NETWORK CODED STORAGE I/O SUBSYSTEM FOR HPC EXASCALE APPLICATIONS</a>
																	<li><hr class="rmBottomMargin"/></li>
																	<li><br></li><li>Manavalan R.<i>"Application level challenges and issues of processing different frequency, polarization and incidence angle Synthetic Aperture Radar data using distributed computing resources"</i></li>
																	<li><hr class="rmBottomMargin"/></li>
																	<li><span class="bold"><br />Abstract:</span>The role and need of Synthetic Aperture Radar (SAR) technology in various geospatial applications is proved beyond the doubts as SAR can make available critical information about on the filed information with centimeter to millimeter accuracy. The science of processing such SAR data using the distributed computing resources is more than two decade old and as of now been broadly bolstered by worldwide HPC labs for example, ESA’s G-POD (Grid Processing on Demand), Peppers (A Performance Emphasized Production Environment for Remote Sensing), DLR distributed SAR data processing environment, Center for Earth Observation and Digital Earth (CEODE) of Chinese Academy of Sciences (CAS),..etc. This paper discusses the difficulties and issues of Geospatial application users who are working with multiple sets of voluminous temporal SAR data and brings out the importance of developing and deploying all-purpose HPC based infrastructure environment that can meet the expectations of application users. Specific situations which require simultaneous processing of different frequency, polarization and incidence angle SAR data using the distributed HPC resources and related requirements will be discussed. For example during the SAR raw data processing, the need and importance of simultaneous processing and extraction of different polarization images as well as in completing its related post processing tasks and mapping operations will be highlighted. When the same has to be done at a regional scale, mainly to simulate large scale disaster events with various multi-look factors as well as with different post processing filtering options the need of developing and deploying such real time solutions on an exascale computing environment can be well understood. As on date such large scale complex SAR application model supporting the above mentioned expectations is yet to be developed. In line to this this talk will brings out the importance of prototyping such HPC based SAR data processing environment which can support the complete application cycle of real time regional disaster management simulations and its related operations. Any such operational setup certainly needs effective coordination of worldwide space agencies as well as distributed HPC labs.</li>
																	<li><span class="bold"><br />Presentation Slides: </span><br />
																	<a href="../slides/13DEC2017-CDACB-Manavaln-Slides.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation">‘Application level’ challenges and issues of processing different frequency, polarization and incidence angle Synthetic Aperture Radar data using distributed computing resources</a><br />
																	</li>
																	<li><hr class="rmBottomMargin"/></li>
																										<li><br></li><li>Venkatesh Shenoi, Janakiraman S and Sandeep Joshi, <i> "Towards energy efficient numerical weather prediction Scalable algorithms and approaches"</i></li>
																										<li><hr class="rmBottomMargin"/></li>
																										<li><span class="bold"><br />Abstract:</span>The weather forecasting has a major impact on the society. The meteorologists have been using the
numerical weather prediction models for the operation forecast for over several decades ever since
Richardson's attempt towards numerical models for weather prediction. The weather codes have
consumed a good portion of the computational power available in several supercomputing centers.
This has led to the need to improve the computational efficiency of the codes/models to achieve
better resolution moving towards the better accuracy of the forecasts. However, these large scale
computations are possible only at the expense of the huge energy budget due to the increasing
requirement of computational resources as well as the cooling infrastructure required to maintain
them. But with the energy budget fixed at 20 MW it is even more challenging as we are compelled
to move towards energy efficiency of the weather codes. We focus towards the spectral transform
method as case study for this talk. This method has been in use in NCAR and ECMWF for the
operational forecasts.<br /><br />

In this talk, we shall discuss the basics of numerical weather predictions and move towards the test
bed shallow water model to be solved by spectral transform method. The highlights of the approach
with regard to algorithm, its scalability and improvements leading to the reduction in the
computational complexity will be discussed. To conclude, some of the recent efforts on scaling up
the solver for shallow water equations are discussed along with the glimpse of the ongoing efforts in
the ESCAPE project. This talk is inspired by our project proposal on “Scalable algorithmic
approach to spectral transform method” to be pursued under NSM, India.</li>
																										<li><span class="bold"><br />Presentation Slides: </span><br />
																										<a href="../slides/venkatesh_shenoi_slides.pdf" class="submitPdf presentationSlides" target="_blank" title="PDF Presentation">Towards Energy Efficient Numerical Weather Prediction</a><br />
																										</li>
																										<li><hr class="rmBottomMargin"/></li>
																	<li><br></li></ul>
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">TBD
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">14:50-15:10
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">Narendra Karmarkar, VCV Rao, <i>"MPPLAB(E-Teacher)"</i>
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">
											</div>
									</div>
									<div class="hideAbove760">
										<div class="col-lg-3">
												<img src="../img/vcv.jpg" class="speakersPic addMarginTopPic" alt="VCV Rao Picture">
										</div>
										<div class="col-lg-9 desc">
												<p class="styleFont fontType justified">
													<span class="bold">Abstract: </span>HPC has huge future scope for scientific applications, large data bases, AI/deep learning, business applications in financial industry, telecom (particularly 5G) etc. Unfortunately, present systems are serving the market with separate products, in a rather fragmented manner. Challenge is to create a "Unified Architecture" that will unify the user space, following a "Top-down" approach instead of “Bottom-up" approach which forces applications to try to fit their code to peculiarities of particular cpu's, accelerators, system architecture etc. Today's potential of HPC and silicon technology is grossly underutilized due to effort and time spent in tailoring parallel code to specific machines.    Incorporation of FPGA-based reconfigurability in general applications is getting delayed unnecessarily. If economic benefits based on what is eminently feasible technologically, are not delivered to the society quickly enough, it slows down investments in further technological enhancements. All HPC users will benefit from this in the long run. At the same time, we are focusing on how very complex code can be put  together in shorter time-span, and in such a way that investment in top-level code design is long-lived in face of  anticipated changes in successive generations of chips, interfaces etc. Due to vast scope, we will present only a broad overview and elaborate only on couple of aspects.  Improving programmer productivity by designing and writing parallel code at multiple levels of abstraction by providing more expressive notations, tools for transforming one level to the next is required. It is also necessary to do away with artificial boundary between hardware description languages and how far traditional compilers reach starting from high level languages. This will ensure more seamless utilization of FPGA-based re-configurability in the unified system architecture. Since hardware aspects are too complex, only one particular aspect related to GPU's will be covered.  Initially, we’ll be addressing application of optimization algorithms to economic modelling and telecom systems.<br />

													<span class="bold"><br />Bio: </span>VCV.Rao received his Master degree in Mathematics from Andhra University, India in 1985 and Ph.D degree in Mathematics from IIT-Kanpur in the year 1993. He is associated with C-DAC since 1993 on High Performance Computing projects. He contributed to design, develop and deploy  C-DAC’s PARAM Series of Supercomputers, GARUDA Grid Computing project, Parallel Computing workshops, contributed to PARAM series at premier academic Institutions. Currently, he is an Associate Director in the High-Performance Computing Technologies (HPC-Tech) Group, at C-DAC, Pune.<br />
												</p>
										</div>
									</div>
									<div class="hideAbove760">
										<div class="col-lg-3">
												<img src="../img/Narendra.jpg" class="speakersPic addMarginTopPic" alt="VCV Rao Picture">
										</div>
										<div class="col-lg-9 desc">
												<p class="styleFont fontType justified"><span class="bold"><br />Bio: </span>Karmarkar received his B.Tech in EE from IIT Bombay in 1978, M.S. from the California Institute of Technology in 1979 and Ph.D. in Computer Science from the University of California, Berkeley in 1983. He is well known for linear programming algorithms - a cornerstone in the field of Linear Programming. He is a Fellow of Bell Laboratories (1987 onwards). In 2006-2007, he served as scientific advisor to the Chairman Tata group, founded CRL and architected "EKA" system, which stands for "Embedded Karmarkar Algorithm”. Currently, he is a Consultant Chief Architect in C-DAC, Pune. He is also a distinguished visiting professor at several institutes such as IISc, and IITs.<br />
												</p>
										</div>
									</div>
									<div class="col-xs-12"><hr class="rmBottomMargin"/></div>
									<div class="col-xs-12 hideBelow500 redBackground">
											<div class="col-xs-2">15:10-15:30
											</div>
											<div class="col-xs-8 center-aligned">COFFEE BREAK & NETWORKING
											</div>
											<div class="col-xs-2 left-aligned">
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont redBackground">
											<div class="col-xs-2 paddingLeftEventTime">15:10-15:30
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">COFFEE BREAK & NETWORKING Talks
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">
											</div>
									</div>
									<div class="col-xs-12"><hr class="rmTopMargin"/></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2 hideBelow500">15:30-17:00
											</div>
											<div class="col-xs-8 left-aligned">Hands-on session: Using the Interactive Parallelization Tool (IPT) to Generate OpenMP, MPI, and CUDA Programs
											</div>
											<div class="col-xs-2 left-aligned">Ritu Arora, TACC
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">15:30-17:00
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">Hands-on session: Using the Interactive Parallelization Tool (IPT) to Generate OpenMP, MPI, and CUDA Programs
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">Ritu Arora, TACC
											</div>
									</div>
									<div class="col-xs-12"><hr /></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">17:00-17:30
											</div>
											<div class="col-xs-8 left-aligned">Parallel programming contest (MPI/OpenMP/CUDA)  - parallelize programs with or without IPT - C/C++ as the base language - prizes for top contestants - The winner award is Nvidia Tesla K40C GPU
											</div>
											<div class="col-xs-2 left-aligned">
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">17:00-17:30
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">Parallel programming contest (MPI/OpenMP/CUDA)  - parallelize programs with or without IPT - C/C++ as the base language - prizes for top contestants - The winner award is Nvidia Tesla K40C GPU
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">
											</div>
									</div>
									<div class="col-xs-12"><hr /></div>
									<div class="col-xs-12 hideBelow500">
											<div class="col-xs-2">17:30-18:00
											</div>
											<div class="col-xs-8 left-aligned">Panel Discussion
											</div>
											<div class="col-xs-2 left-aligned">TBD
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 styleFont">
											<div class="col-xs-2 paddingLeftEventTime">17:30-18:00
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">Panel Discussion
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">TBD
											</div>
									</div>
									<div class="col-xs-12"><hr class="rmBottomMargin"/></div>
									<div class="col-xs-12 hideBelow500 redBackground">
											<div class="col-xs-2">18:00-19:00
											</div>
											<div class="col-xs-8 center-aligned">NETWORKING RECEPTION AND PRIZE WINNER ANNOUNCED
											</div>
											<div class="col-xs-2 left-aligned">
											</div>
									</div>
									<div class="col-xs-12 hideAbove500 redBackground styleFont">
											<div class="col-xs-2 paddingLeftEventTime">18:00-19:00
											</div>
											<div class="col-xs-8 left-aligned paddingLeftEventTitle">NETWORKING RECEPTION AND PRIZE WINNER ANNOUNCED
											</div>
											<div class="col-xs-2 left-aligned paddingLeftSpeaker">
											</div>
									</div>
									<div class="col-xs-12"><hr class="rmTopMargin"/></div>
								</div>
							</div>
						</div>

  <a name="datesOption"></a>
	<div class="topicsSec onlyBorder">
    <div class="container-fluid theDate fontType">
        <div class="theBorder container">
					<br />
      <h3 class="bold">Important Dates</h3>
        <ul class="noList">
         <li>Registration opens on: November 1, 2017</li>
         <li>Abstract Submission Deadline: November 15, 2017 (original), November 20, 2017 (extended)</li>
         <li>Acceptance Notification: November 25, 2017 </li>
         <li>Registration with fee waiver closes on: December 7, 2017</li>
         <li>Final submission deadline: December 7, 2017</li>
         <li>Workshop will be held on: December 17, 2017</li>
        </ul>
        <br/>
    </div>
  </div>
</div>
 <div class="row row1">
<div class="col-md-6 backgroundBlue">
  <a name="committeeOption"></a>
  <div class="container-fluid thecommitte theCommitte992Above hideAgendaTitle-md">
       <h2 class="bold">Committee</h2><br />
        <h4 class="bold">Workshop Chairs</h4>
        <ul class="noList">
          <li><a href="https://www.tacc.utexas.edu/about/directory/ritu-arora" class="none">Ritu Arora</a>,<a href="https://www.tacc.utexas.edu" class="none">Texas Advanced Computing Center, USA</a></li>
          <li>Sharda Dixit, Centre for Development of Advanced Computing, India</li>
        </ul>


         <br /><h4 class="bold">Publicity</h4>
         <ul class="noList">
          <li>Nitin Sukhija, Slippery Rock University of Pennsylvania, USA</li>
          <li>V Venkatesh Shenoi, Centre for Development of Advanced Computing, India</li>
         </ul>


        <br /><h4 class="bold">Program Committee</h4>
        <ul class="noList">
            <li>Amarjeet Sharma, Centre for Development of Advanced Computing, India</li>
            <li>Amit Majumdar, San Diego Supercomputing Center, USA</li>
            <li>Anil Kumar Gupta, Centre for Development of Advanced Computing, India</li>
            <li>Antonio Iglesias Gomez, Oak Ridge National Laboratory, USA</li>
            <li>Carlos Rosales Fernandez, Intel, USA</li>
            <li>Lars Koesterke, Texas Advanced Computing Center, USA</li>
            <li>Sandeep K Joshi, Centre for Development of Advanced Computing, India</li>
	    <li>Seetha Ram Krishna Nookala, Intel, India</li>
            <li>Shreya Bokare, Centre for Development of Advanced Computing, India</li>
            <li>Soham Ghosh, Centre for Development of Advanced Computing, India</li>
            <li>Suman Roychoudhury, Tata Consultancy Services, India</li>
            <li>VCV Rao, Centre for Development of Advanced Computing, India</li>
            <li>Vinodh Kumar M, Centre for Development of Advanced Computing, India</li>

        </ul>

				<br /><h4 class="bold">Webmaster</h4>
				<ul class="noList">
				 <li><a href="http://geraldjoshua.com" class="none">Gerald Joshua</a>, Texas Advanced Computing Center, USA</li>
				</ul>
    </div>
		<div class="container-fluid thecommitte theCommitte992Below hideAgendaTitle-lg-sm">
	       <h2 class="bold">Committee</h2><br />
	        <h4 class="bold">Workshop Chairs</h4>
	        <ul class="noList">
	          <li><a href="https://www.tacc.utexas.edu/about/directory/ritu-arora" class="none">Ritu Arora</a>,<a href="https://www.tacc.utexas.edu" class="none">Texas Advanced Computing Center, USA</a></li>
	          <li>Sharda Dixit, Centre for Development of Advanced Computing, India</li>
	        </ul>


	         <br /><h4 class="bold">Publicity</h4>
	         <ul class="noList">
	          <li>Nitin Sukhija, Slippery Rock University of Pennsylvania, USA</li>
	          <li>V Venkatesh Shenoi, Centre for Development of Advanced Computing, India</li>
	         </ul>


	        <br /><h4 class="bold">Program Committee</h4>
	        <ul class="noList">
	         <li>Amarjeet Sharma, Centre for Development of Advanced Computing, India</li>
                 <li>Amit Majumdar, San Diego Supercomputing Center, USA</li>
                 <li>Anil Kumar Gupta, Centre for Development of Advanced Computing, India</li>
                 <li>Antonio Iglesias Gomez, Oak Ridge National Laboratory, USA</li>
                 <li>Carlos Rosales Fernandez, Intel, USA</li>
                 <li>Lars Koesterke, Texas Advanced Computing Center, USA</li>
                 <li>Sandeep K Joshi, Centre for Development of Advanced Computing, India</li>
	         <li>Seetha Ram Krishna Nookala, Intel, India</li>
                 <li>Shreya Bokare, Centre for Development of Advanced Computing, India</li>
                 <li>Soham Ghosh, Centre for Development of Advanced Computing, India</li>
                 <li>Suman Roychoudhury, Tata Consultancy Services, India</li>
                 <li>VCV Rao, Centre for Development of Advanced Computing, India</li>
                 <li>Vinodh Kumar M, Centre for Development of Advanced Computing, India</li>


	        </ul>

					<br /><h4 class="bold">Webmaster</h4>
					<ul class="noList">
					 <li><a href="http://geraldjoshua.com" class="none">Gerald Joshua</a>, Texas Advanced Computing Center, USA</li>
					</ul>
	    </div>
 </div>
<div class="col-md-6 marginLeft">
  <a name="sponsorsOption"></a>
	<br />
      <div class="container-fluid marginSponsors hidemax376">
      <h2 class="bold curSponsorTitle">Current Sponsors</h2>
				<div class="row intelLogoWrapper"><a href="https://www.intel.com/" target="_blank"><img src="../img/intel.jpg" class="intelLogoOnSponsors" alt="intel"></a></div><br />
				<div class="row"><div class="col-xs-6"><a href="http://www.nvidia.com/" target="_blank"><img src="../img/NVLogo_2D.jpg" class="nvidiaLogo" alt="intel"></a></div></div><br />
				<div class="row">
        <a href="https://www.cdac.in/" target="_blank"><div class="col-xs-1 sponsorMargin1"><img class="sponsorLogos" src="../img/cdac-logo.jpg"></div></a>
				<a href="https://www.nsf.gov/" target="_blank"><div class="col-xs-1 sponsorMargin"><img class="sponsorLogos" src="../img/nsf.jpg"></div></a>
        <a href="https://nsmindia.in/" target="_blank"><div class="col-xs-1 sponsorMargin"><img class="sponsorLogos" src="../img/nsm.jpg"></div></a>
        <a href="https://www.tacc.utexas.edu/" target="_blank"><div class="col-xs-1 sponsorMargin"><img class="sponsorLogos" src="../img/tacc.jpg"></div></a>
				</div><br />
		</div>
		<div class="container-fluid marginSponsors hidemin376">
		<h2 class="bold curSponsorTitle">Current Sponsors</h2>
			<div class="row intelLogoWrapper-sm"><a href="https://www.intel.com/" target="_blank"><img src="../img/intel.jpg" class="intelLogoOnSponsors-sm" alt="intel"></a></div><br />
			<div class="row"><div class="col-xs-6"><a href="http://www.nvidia.com/" target="_blank"><img src="../img/NVLogo_2D.jpg" class="nvidiaLogo" alt="nvidia"></a></div></div><br />
			<div class="row">
			<div class="row"><div class="col-xs-6"><a href="https://www.cdac.in/" target="_blank"><img class="sponsorsLogo-sm" src="../img/cdac-logo.jpg"></div></div></a>
			<div class="row"><div class="col-xs-6"><a href="https://www.nsf.gov/" target="_blank"><img class="sponsorsLogo-sm" src="../img/nsf.jpg"></div></div></a>
			<div class="row"><div class="col-xs-6"><a href="https://nsmindia.in/" target="_blank"><img class="sponsorsLogo-sm" src="../img/nsm.jpg"></div></div></a>
			<div class="row"><div class="col-xs-6"><a href="https://www.tacc.utexas.edu/" target="_blank"><img class="sponsorsLogo-sm" src="../img/tacc.jpg"></div></div></a>
			</div><br />
	</div>

		  <div class="container-fluid">
      <h2 class="bold">Sponsorship Levels</h2>
      <ul class="noList fontType justified">
      <li><span class="bold"><u>Platinum Level:</u></span><br />US $7000, one speaker-slot, large-sized logo in the header area of the website, name displayed prominently on the signage in workshop area, promotion through workshop advertisements, 10 complimentary registrations, URL to the sponsor website added to the workshop website, up to ten products and services highlighted through blogs on the workshop forum page, invitation on a panel</li>
      <li><span class="bold"><u>Gold Level:</u></span> <br />US $5000, medium-sized logo on the website, signage in workshop area, promotion through workshop advertisements, up to five products and services highlighted through blogs on the workshop forum page, invitation on a panel, 5 complimentary registrations </li>
      <li><span class="bold"><u>Silver Level:</u></span> <br />US $3000, logo on the website, promotion through workshop advertisements, one product or service highlighted through a blog on the workshop forum page, 3 complimentary registrations </li>
      <li><span class="bold"><u>Bronze Level:</u></span> <br />US $2000, logo on the website, promotion through workshop advertisements, 2 complimentary registrations </li>
      </ul>
     </div>
	 </div></div>

</div>
<a name="CFAOption"></a>
<div class="col-md-12 onlyBorder2">
<div class="theBorder container marginTop">
			<h3 class="bold makeItCenter">CFA (Call For Abstracts)</h3>
			<br />
			<p class="fontType justified"> The SCEC workshop will include a track for lightning talks during which ten-minute talks will be presented in a sequence. The talks will provide a high-level overview of the topics that are aligned with the theme of the workshop. While the time allocated for the lightening talks may not be enough for presenting the fine details of the chosen topic, it could be enough for including key information that piques the interest of the audience for an engaging discussion after the talk. The abstract and slides of the talks will be published on the workshop website. There can be multiple authors on a submission but only one presenter is permitted for each lightning talk due to time-constraints.
<br/><br/><b>Guidelines for preparing the submission for the lightning talk are as follows:</b><br /><br />
In maximum 300 words, the abstract should answer the following questions on a topic that is relevant to the workshop:<br /></p>
<ul class="noList fontType justified">
<li><span class="bold">• What is the software challenge/problem that is being solved?</span></li>
<li><span class="bold">• Why is this challenge/problem important to the HPC and/or the advanced software engineering community?</li>
<li><span class="bold">• If applicable, who will benefit from the software/approach?</span></li>
<li><span class="bold">• If applicable, what is the novelty of the software/approach?</span></li>
<li><span class="bold">• If applicable, how do the preliminary results compare to those of the related work?</span></li>
</ul><br />
<p class="fontType justified">A rough draft of the proposed presentation (up to 5 slides) should also be submitted along with the abstract.
The abstracts and the slides must be submitted in the PDF format through the submission system at the following URL: <a href="https://easychair.org/conferences/?conf=scec17" target="_blank" class="submitPdf hideAgendaTitle-max399" title="Submit PDF">https://easychair.org/conferences/?conf=scec17
</a><a href="https://easychair.org/conferences/?conf=scec17" target="_blank" class="submitPdf hideAgendaTitle-min399 styleFont2" title="Submit PDF">https://easychair.org/conferences/?conf=scec17
</a></p>
<span class="abstract">*</span>Abstracts will not be accepted over email. Abstracts which are incomplete or received after the deadline will not be considered. The submission system will close on <b>November 20, 2017</b>.
	<p><br/><span class="bold fontType">Presentation guidelines:</p class="fontType">
Lightning talk presentations are limited to one per speaker. Co-authors are not included in this rule. A person can be a co-author on any number of abstracts.</p>
</div></div>


<div class="col-md-12 onlyBorder">
<a name="registrationOption"></a>
<div class="theBorder container marginTop">
      <h3 class="bold makeItCenter"> Registration</h3>

      <p class="fontType"> The registration fees for the workshop is Rupees 1000 (US $16) and can be <b><u>paid at the venue</u></b> using cash or credit/debit card. <b> With the support of our sponsors, we are able to waive off the fee for a selected number of particpants who are registered by December 7, 2017</b>. For requesting the fee waiver, please send an email at "scecforum@gmail.com" with the subject line "Registration Fee Waiver" and let us know how the fee waiver will help you. All the workshop attendees should register in advance by filling the following form: </p>
			<div class="makeItCenter">
			<a href="https://tinyurl.com/yc2sglyv" target="_blank" class="btn btn-lg" title="Registration"> Registration Form
			</a></div><hr style="height:2px;border:none;color:#333;background-color:#333;"/>
			<p class="fontType">  <h3 class="bold makeItCenter"> Workshop Venue</h3> <div class="makeItCenter fontType"><h4>Hotel Royal Orchid</h4>Opposite to BSNL Office, Near Durgapura Flyover, Tonk Road, Durgapura, Jaipur Rajasthan India 302018</div></p>
  </div></div>

<div class="col-md-12">
  <a name="contactOption"></a>
    <div class="row contact fontType">
			<div class="container-fluid makeItCenter">
      <h3> Contact</h3>

      <p>For any questions regarding the workshop, please contact us at: scecforum@gmail.com</p><br />
    </div>
</div>
</div>
  	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  	<script>
		//Once user scrolls down or up, the background of the top navbar will change and
			//the navbar will still be on the top of the page
		$(function () {
			  $(document).scroll(function () {
				    var fixedTopNavBar = $("#navbarTop");
						var intelLogoTop = $("#theIntelLogo");
				    fixedTopNavBar.toggleClass('scrolled', $(this).scrollTop() > fixedTopNavBar.height());
						if($(this).scrollTop() > fixedTopNavBar.height()){
								intelLogoTop.css("width", "5em");
								intelLogoTop.css("height", "3em");
						}
						else{
								intelLogoTop.css("width", "7.5em");
								intelLogoTop.css("height", "5em");
						}

			  });
		});

		//Navigate which option users click
		$("#navbarTop").each(function(){
				$("li").click(function(){
						scrollToAnchor(this.id);
				});
		});

		//Scroll to the section that users are interested in
		function scrollToAnchor(theName){
		    var aTag = $("a[name='"+ theName +"']");
		    $('html,body').animate({scrollTop: aTag.offset().top},'slow');
		}
  	</script>
</body>
</html>
